{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electric Energy Load Demand Forecasting for Rural Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Weather time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read load data \n",
    "def data_reader(file_name):\n",
    "    data = pd.read_excel(file_name, parse_dates=True, \n",
    "                         index_col='Time', usecols=range(2))\n",
    "    return data\n",
    "\n",
    "# function to read weather data\n",
    "def weather_reader(file_name):\n",
    "    weather = pd.read_excel(file_name, parse_dates=True, \n",
    "                            index_col='Time measured')\n",
    "    return weather\n",
    "\n",
    "# function for concatenating load data and weather data for training\n",
    "def concat_data(file_name_load, file_name_weather):\n",
    "    train_data = pd.concat([file_name_load, file_name_weather], axis=1)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather & load time-series data\n",
    "load_data = data_reader('Index_Bjønntjønn_2014_2018.xlsx')\n",
    "weather_data = weather_reader('bo_temp_2014_2018.xlsx')\n",
    "weather_data = weather_data.interpolate()\n",
    "\n",
    "# Concatenate\n",
    "dataframe = concat_data(load_data, weather_data)\n",
    "\n",
    "# Renaming columns for easier interpreting:\n",
    "dataframe = dataframe.rename(columns={\"Total\":\"Load\",\n",
    "                                      \"Middeltemperatur i 2m høyde (TM)\": \"Temperature\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(data, time_start, time_end=None):\n",
    "    # Ploting time-series data with different time ranges\n",
    "    fig, ax = plt.subplots(figsize=(7,4.5))\n",
    "    ax2 = ax.twinx()\n",
    "    data['Load'].loc[time_start:time_end].plot(\n",
    "        c='seagreen', label='Load', ax=ax)\n",
    "    if time_end is None:\n",
    "        data['Temperature'].loc[time_start].plot(\n",
    "            c='darkorange', label='Temperature', ax=ax2)\n",
    "    else:\n",
    "        data['Temperature'].loc[time_start:time_end].plot(\n",
    "            c='darkorange', label='Temperature', ax=ax2)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax.set_ylabel('Load', fontsize=12, fontweight='bold', \n",
    "                  color='seagreen')\n",
    "    ax2.set_ylabel('Temperature', fontsize=12, \n",
    "                   fontweight='bold', color='darkorange')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series for 2018\n",
    "show_plots(dataframe, '2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series for June to August 2018\n",
    "show_plots(dataframe, '2018-06', '2018-08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots(dataframe, '2018-06-01', '2018-06-07')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate multi-step load forecasting with feed-forward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coding holiday dates\n",
    "holiday_dates = {\n",
    "    # 2014 year\n",
    "    '2014-Jan-1st': ('2014-01-01', None),  # single day\n",
    "    '2014-Easter': ('2014-04-14', '2014-04-21'),  # date range\n",
    "    '2014-May-1st': ('2014-05-01', None),\n",
    "    '2014-Pentecost': ('2014-06-07', '2014-06-10'),\n",
    "    '2014-Xmas': ('2014-12-21', '2014-12-31'),\n",
    "    # 2015 year\n",
    "    '2015-Jan-1st': ('2015-01-01', None),\n",
    "    '2015-Easter': ('2015-03-30', '2015-04-06'),\n",
    "    '2015-May-1st': ('2015-05-01', None),  # Friday\n",
    "    '2015-Ascension': ('2015-05-14', None),\n",
    "    '2015-Pentecost': ('2014-05-24', '2014-05-25'),\n",
    "    '2015-Xmas': ('2015-12-23', '2015-12-31'),\n",
    "    # 2016 year\n",
    "    '2016-Jan-1st': ('2016-01-01', None),\n",
    "    '2016-Easter': ('2016-03-21', '2016-03-28'),\n",
    "    '2016-May-1st': ('2015-05-01', None),  # Sunday\n",
    "    '2016-Ascension': ('2016-05-05', None),\n",
    "    '2016-Pentecost': ('2016-05-16', '2016-05-17'),\n",
    "    '2016-Xmas': ('2016-12-26', '2016-12-31'),\n",
    "    # 2017 year\n",
    "    '2017-Jan-1st': ('2017-01-01', None),\n",
    "    '2017-Easter': ('2017-04-10', '2017-04-17'),\n",
    "    '2017-May-1st': ('2017-05-01', None),  # Monday\n",
    "    '2017-May-17th': ('2017-05-17', None),  # Wednesday\n",
    "    '2017-Ascension': ('2017-05-25', None),\n",
    "    '2017-Pentecost': ('2017-06-05', None),\n",
    "    '2017-Xmas': ('2017-12-25', '2017-12-31'),\n",
    "    # 2018 year\n",
    "    '2018-Jan-1st': ('2018-01-01', None),\n",
    "    '2018-Easter': ('2018-03-26', '2018-04-02'),\n",
    "    '2018-May-1st': ('2018-05-01', None),  # Tuesday\n",
    "    '2018-Ascension': ('2017-05-10', None),  # Thursday\n",
    "    '2018-May-17th': ('2017-05-17', None),\n",
    "    '2018-Pentecost': ('2018-05-21', None),\n",
    "    '2018-Xmas': ('2018-12-24', '2018-12-31')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(dataframe, holiday_dates, columns, time_lags=24, \n",
    "                      steps_ahead=1, drop_nan_rows=True):\n",
    "    \"\"\"Engineering features\n",
    "    \n",
    "    Load data features column names with underscore (i.e. Load_1h, Load_2h, etc.) \n",
    "    represent time-lags (t-1, t-2, ...), while those with plus sign (i.e. Load+1h, \n",
    "    Load+2h, etc.) represent future time-steps (t+1, t+2, ...); column with name\n",
    "    Load+0h represents current load at time instant t. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pd.DataFrame\n",
    "        original dataframe with time-series data\n",
    "    holiday_dates: dictionary\n",
    "        dictionary with tuples specifying local holiday dates or date-ranges\n",
    "    columns: list\n",
    "        list of column names from the dataframe which are used for the \n",
    "        features engineering (i.e. time-lags)\n",
    "    time_lags: int\n",
    "        number of time lags for use with feature engineering\n",
    "    steps_ahead: int\n",
    "        number of steps ahead for multi-step forecasting (steps_ahead=1\n",
    "        means single-step ahead forecasting)\n",
    "    drop_nan_rows: bool\n",
    "        True/False indicator to drop rows with NaN values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe: pd.DataFrame\n",
    "        dataframe augmented with additional features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the original dataframe\n",
    "    data = dataframe[columns].copy()\n",
    "            \n",
    "    # Features engineering\n",
    "    for col in data.columns:\n",
    "        for i in range(1, time_lags+1):\n",
    "            # Shift data by lag of 1 to time_lags (default: 24) hours\n",
    "            data[col+'_{:d}h'.format(i)] = data[col].shift(periods=i)  # time-lag\n",
    "        data[col+'_diff'] = data[col].diff()  # first-difference\n",
    "        data[col+'_week'] = data[col].shift(periods=24*7)  # previous week\n",
    "    \n",
    "    # Hour-of-day indicators with cyclical transform\n",
    "    dayhour_ind = data.index.hour\n",
    "    data['hr_sin'] = np.sin(dayhour_ind*(2.*np.pi/24))\n",
    "    data['hr_cos'] = np.cos(dayhour_ind*(2.*np.pi/24))\n",
    "    \n",
    "    # Day-of-week indicators with cyclical transform\n",
    "    weekday_ind = data.index.weekday\n",
    "    data['week_sin'] = np.sin(weekday_ind*(2.*np.pi/7))\n",
    "    data['week_cos'] = np.cos(weekday_ind*(2.*np.pi/7))\n",
    "\n",
    "    # Weekend as a binary indicator\n",
    "    data['weekend'] = np.asarray([0 if ind <= 4 else 1 for ind in weekday_ind])\n",
    "\n",
    "    # Month indicators with cyclical transform\n",
    "    month_ind = data.index.month\n",
    "    data['mnth_sin'] = np.sin((month_ind-1)*(2.*np.pi/12))\n",
    "    data['mnth_cos'] = np.cos((month_ind-1)*(2.*np.pi/12))\n",
    "    \n",
    "    # Holidays as a binary indicator\n",
    "    data['holidays'] = 0\n",
    "    for holiday, date in holiday_dates.items():\n",
    "        if date[1] is None:\n",
    "            # Single day\n",
    "            data.loc[date[0], 'holidays'] = 1\n",
    "        else:\n",
    "            # Date range\n",
    "            data.loc[date[0]:date[1], 'holidays'] = 1\n",
    "    \n",
    "    # Forecast horizont\n",
    "    if steps_ahead == 1:\n",
    "        # Single-step forecasting\n",
    "        data['Load+0h'] = data['Load'].values\n",
    "    else:\n",
    "        # Multi-step forecasting\n",
    "        for i in range(steps_ahead):\n",
    "            data['Load'+'+{:d}h'.format(i)] = data['Load'].shift(-i)\n",
    "    del data['Load']\n",
    "    \n",
    "    if drop_nan_rows:\n",
    "        # Drop rows with NaN values\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataframe, start_date, window_days=100, train_percent=80.,\n",
    "                     return_arrays=False):\n",
    "    \"\"\"Train and test data set split\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pd.DataFrame\n",
    "        dataframe augmented with additional features\n",
    "    start_date: string\n",
    "        starting date of the time-series \n",
    "    window_days: int\n",
    "        size of the data window in days\n",
    "    train_percent: float\n",
    "        percentage of the data window size to use for creating the \n",
    "        training data set (the rest is used for testing)\n",
    "    return_arrays: bool\n",
    "        True/False indicator which defines the type of output; if \n",
    "        True function returns numpy arrays; if False it returns\n",
    "        pandas dataframes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train: pd.DataFrame or np.array\n",
    "        training data 2D array of input features\n",
    "    y_train: pd.DataFrame or np.array\n",
    "        training data array of target values\n",
    "    X_test: pd.DataFrame or np.array\n",
    "        testing/validation data 2D array of input features\n",
    "    y_test: pd.DataFrame or np.array\n",
    "        testing/validation data array of target values\n",
    "    \"\"\"\n",
    "    data = dataframe.copy()\n",
    "    if window_days*24 > data.values.shape[0]:\n",
    "        raise ValueError('Variable window_days has too large value: {}*24h = {} > {}, \\\n",
    "            which is more than there is data!'.format(window_days, window_days*24, \n",
    "                                                      data.values.shape[0]))\n",
    "    \n",
    "    # Split dataframe into X, y\n",
    "    columns = data.columns.values\n",
    "    outputs = [col_name for col_name in columns if 'Load+' in col_name]\n",
    "    inputs = [col_name for col_name in columns if col_name not in outputs]\n",
    "    # inputs (features)\n",
    "    X = data[inputs]\n",
    "    # outputs\n",
    "    y = data[outputs]\n",
    "    \n",
    "    # Training period\n",
    "    train_percent = train_percent/100.\n",
    "    st = pd.to_datetime(start_date)  # start date\n",
    "    et = st + dt.timedelta(days=int(train_percent*window_days))  # end date\n",
    "    X_train = X.loc[st:et]\n",
    "    y_train = y.loc[st:et]\n",
    "    \n",
    "    # Testing / Validation period\n",
    "    sv = et \n",
    "    ev = sv + dt.timedelta(days=int((1-train_percent)*window_days)+1)\n",
    "    X_test = X.loc[sv:ev]\n",
    "    y_test = y.loc[sv:ev]\n",
    "        \n",
    "    if return_arrays:\n",
    "        # Returning numpy arrays\n",
    "        return X_train.values, y_train.values, X_test.values, y_test.values\n",
    "    else:\n",
    "        # Returning pandas dataframes\n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day-ahead short-term load forecasting with time-series data\n",
    "STEPS_AHEAD = 24           # hours-ahead for prediction\n",
    "START_DATE = '2014-01-09'  # starting date of the time-series\n",
    "WINDOW_SIZE_DAYS = 400     # window size in days for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day-ahead (i.e. 24-hours ahead) forecasting\n",
    "data_features = engineer_features(dataframe, holiday_dates, \n",
    "                                  columns=['Load', 'Temperature'], \n",
    "                                  steps_ahead=STEPS_AHEAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test/validation sets\n",
    "X_train, y_train, X_test, y_test = train_test_split(\n",
    "    data_features, \n",
    "    start_date=START_DATE, \n",
    "    window_days=WINDOW_SIZE_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and transform input data\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256  # batch size for training\n",
    "EPOCHS = 400      # epochs for training\n",
    "WAIT = 50         # patience for early stopping\n",
    "LR = 1e-3         # initial learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed-forward deep ANN using functional `tf.keras` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward and fixed funnel-shaped deep ANN\n",
    "# tf.keras functional API\n",
    "input_layer = keras.layers.Input(shape=X_train_sc.shape[1:])\n",
    "x = keras.layers.Dense(units=1024, activation='relu')(input_layer)\n",
    "x = keras.layers.Dropout(0.2)(x)  # regularization\n",
    "x = keras.layers.Dense(units=512, activation='relu')(x)\n",
    "x = keras.layers.Dense(units=512, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.1)(x)  # regularization\n",
    "x = keras.layers.Dense(units=256, activation='relu')(x)\n",
    "output_layer = keras.layers.Dense(STEPS_AHEAD)(x)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "adam = keras.optimizers.Adam(learning_rate=LR, decay=LR/EPOCHS)\n",
    "model.compile(loss='mae', optimizer=adam, metrics=['mae', 'mape'])\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, \n",
    "                                           patience=WAIT, verbose=1,\n",
    "                                           restore_best_weights=True)\n",
    "history = model.fit(X_train_sc, y_train.values, \n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    validation_data=(X_test_sc, y_test.values),\n",
    "                    callbacks=[early_stop], shuffle=True, verbose=0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "print('MAE val_loss at final epoch is {:.2f}, while min. val_loss is {:.2f}.'\n",
    "      .format(val_loss[-1], min(val_loss)))\n",
    "plt.plot(loss, label='train')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(dataframe, start_date, window_days, test_size=1):\n",
    "    \"\"\" Prepare test data\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe: pd.DataFrame\n",
    "        original dataframe with features\n",
    "    start_date: string\n",
    "        starting date for the time-series previously used\n",
    "        in creating the train and test/validation data sets\n",
    "    window_days: int\n",
    "        size of the data window in days previously used\n",
    "        in creating the train and test/validation data sets\n",
    "    test_size: int\n",
    "        number of time-steps (hours) for walk-forward testing\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_new: np.array\n",
    "        walk-forward testing data set as numpy array\n",
    "    \"\"\"\n",
    "    data = dataframe.copy()\n",
    "    date_test_start = pd.to_datetime(start_date) + dt.timedelta(days=window_days)\n",
    "    date_test_end = date_test_start + dt.timedelta(hours=test_size)\n",
    "    \n",
    "    columns = data.columns.values\n",
    "    outputs = [col_name for col_name in columns if 'Load+' in col_name]\n",
    "    inputs = [col_name for col_name in columns if col_name not in outputs]\n",
    "    \n",
    "    if test_size == 1:\n",
    "        # Array with a single value needs to be reshaped accordingly\n",
    "        X_new = data[inputs].loc[date_test_start].values.reshape(1,-1)\n",
    "    else:\n",
    "        X_new = data[inputs].loc[date_test_start:date_test_end].values[:-1]\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 12  # walk-forward for 12 hours\n",
    "X_new = prepare_test_data(data_features, START_DATE, WINDOW_SIZE_DAYS, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform values using scaler\n",
    "X_new_sc = scaler.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new values using model\n",
    "y_pred = model.predict(X_new_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    \"\"\"Mean absolute percentage error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.array\n",
    "        array holding true values\n",
    "    y_pred: np.array\n",
    "        array holding predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mape: float\n",
    "        mean absolute percentage error value\n",
    "    \"\"\"\n",
    "    if len(y_true.shape) > 1 or len(y_pred.shape) > 1:\n",
    "        raise TypeError('Arrays need to be one-dimensional.')\n",
    "        \n",
    "    mape = np.mean(np.abs((y_true - y_pred)/y_true))*100.\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multi-step ahead predictions using the walk-forward method\n",
    "if TEST_SIZE == 1: \n",
    "    raise ValueError('TEST_SIZE: Need a multi-step ahead predictions!')\n",
    "    \n",
    "date_start = pd.to_datetime(START_DATE) + dt.timedelta(days=WINDOW_SIZE_DAYS)\n",
    "for i in range(TEST_SIZE):\n",
    "    date_end = date_start + dt.timedelta(hours=23)\n",
    "    y_true = data_features['Load+0h'].loc[date_start:date_end]\n",
    "    y_values = pd.DataFrame(y_true)\n",
    "    y_values = y_values.rename(columns={'Load+0h':'Actual'})\n",
    "    y_values['Predicted'] = y_pred[i,:]\n",
    "    \n",
    "    # Absolute percentage error\n",
    "    y_values['APE'] = np.abs((y_values['Actual'] \n",
    "                              - y_values['Predicted'])/y_values['Actual'])*100.\n",
    "    # Mean absolute percentage error\n",
    "    mape = mean_absolute_percentage_error(y_values['Actual'].values, \n",
    "                                          y_values['Predicted'].values)\n",
    "    print('MAPE = {:.2f} (%)'.format(mape))\n",
    "    \n",
    "    # Plot figure\n",
    "    y_values[['Actual', 'Predicted']].plot(figsize=(5.5,3.5))\n",
    "    plt.ylabel('Load')\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    date_start = date_start + dt.timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate multi-step load forecasting with convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_timesteps_features(dataframe, columns, start_date, timesteps=72, \n",
    "                               steps_ahead=24, window_days=100, train_percent=80.):\n",
    "    \"\"\"\n",
    "    Restructure original dataframe with columns time-series data into a 3D array\n",
    "    of shape: [samples, timesteps, features] for the use with convolutional layers.\n",
    "    First dimension of the output array is the number of samples, which is determined\n",
    "    automatically from the size of the window. Second dimension is determined by the \n",
    "    number of timesteps and defines how many time steps from the past will be used in\n",
    "    the internal processing of the convolutional layer. The third dimension is the\n",
    "    number of features in the original dataset, which is defined by the number of \n",
    "    columns that are used from the original dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pd.DataFrame\n",
    "        dataframe with the original time-series data\n",
    "    columns: list\n",
    "        list of column names from the dataframe which are used\n",
    "    start_date: string\n",
    "        starting date of the time-series \n",
    "    timesteps: int\n",
    "        number of time steps from the past for creating output arrays\n",
    "    steps_ahead: int\n",
    "        number of time steps into the future for making predictions\n",
    "    window_days: int\n",
    "        size of the data window in days\n",
    "    train_percent: float\n",
    "        percentage of the data window size to use for creating the \n",
    "        training data set (the rest is used for testing)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean_std_values: dictionary\n",
    "        dictionary with tuples holding mean value and standard\n",
    "        deviation for each of the columns in the dataframe\n",
    "    X_train: np.array\n",
    "        training data 2D array of features\n",
    "    y_train: np.array\n",
    "        training data array of targets\n",
    "    X_test: np.array\n",
    "        testing/validation data 2D array of features\n",
    "    y_test: np.array\n",
    "        testing/validation data array of targets \n",
    "    \"\"\"\n",
    "    \n",
    "    def overlap_windows(dataset, timesteps, steps_ahead):\n",
    "        \"\"\" Create overlaping window of time-series data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: pd.DataFrame\n",
    "            time-series pandas dataset\n",
    "        timesteps: int\n",
    "            number of time steps from the past for creating output arrays\n",
    "        steps_ahead: int\n",
    "            number of time steps into the future for making predictions\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X, y: np.array\n",
    "            input and output 3-d arrays of overlaping time windows\n",
    "        \"\"\"\n",
    "        X = []; y = []\n",
    "        \n",
    "        start = 0\n",
    "        for i in range(len(dataset)):\n",
    "            # Define the end of the input sequence\n",
    "            in_end = start + timesteps\n",
    "            out_end = in_end + steps_ahead\n",
    "            # Ensure that there is enough data\n",
    "            if out_end <= len(dataset):\n",
    "                X.append(dataset[start:in_end, :])\n",
    "                y.append(dataset[in_end:out_end, 0])\n",
    "            # Move along one time step\n",
    "            start += 1\n",
    "            \n",
    "        # Convert list to np.array\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "\n",
    "    data = dataframe.copy()\n",
    "    \n",
    "    if window_days*24 > data.values.shape[0]:\n",
    "        raise ValueError('Variable window_days has too large value: {}*24h = {} > {}, \\\n",
    "            which is more than there is data!'.format(window_days, window_days*24, \n",
    "                                                      data.values.shape[0]))\n",
    "    \n",
    "    # Training period\n",
    "    # ---------------\n",
    "    train_percent = train_percent/100.\n",
    "    st = pd.to_datetime(start_date)  # start date\n",
    "    et = st + dt.timedelta(days=int(train_percent*window_days))  # end date\n",
    "    train = data.loc[st:et].values\n",
    "    \n",
    "    # Standardize and transform training data set\n",
    "    mean_std_values = {}\n",
    "    for i, column in enumerate(columns):\n",
    "        # Calculate mean and standard deviation only\n",
    "        # from the training data set values\n",
    "        mu = train[:,i].mean()  # axis=0\n",
    "        sd = train[:,i].std()\n",
    "        mean_std_values[column] = (mu, sd)\n",
    "        # Standardize training data\n",
    "        train[:,i] = (train[:,i] - mu)/sd\n",
    "    \n",
    "    # Create overlapping windows with training data\n",
    "    X_train, y_train = overlap_windows(train, timesteps, steps_ahead)\n",
    "    \n",
    "    # Testing / Validation period\n",
    "    # ---------------------------\n",
    "    sv = et \n",
    "    ev = sv + dt.timedelta(days=int((1-train_percent)*window_days)+1)\n",
    "    test = data.loc[sv:ev].values\n",
    "    \n",
    "    # Transform testing/validation data set\n",
    "    for i, column in enumerate(columns):\n",
    "        # Use mean and standard deviation from the\n",
    "        # training data set\n",
    "        mu = mean_std_values[column][0]\n",
    "        sd = mean_std_values[column][1]\n",
    "        # Standardize test data\n",
    "        test[:,i] = (test[:,i] - mu)/sd\n",
    "    \n",
    "    # Create overlaping windows with test data\n",
    "    X_test, y_test = overlap_windows(test, timesteps, steps_ahead)\n",
    "    \n",
    "    return mean_std_values, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day-ahead (i.e. 24-hours ahead) short-term load prediction with time-series data\n",
    "START_DATE = '2014-01-09'  # starting date of the time-series data\n",
    "HISTORY_SIZE = 7*24        # window size in hours for the past history values\n",
    "STEPS_AHEAD = 24           # hours-ahead for prediction\n",
    "WINDOW_SIZE_DAYS = 300     # window size in days for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Validation and Testing data sets\n",
    "mean_std_values, X_train, y_train, X_test, y_test = samples_timesteps_features(\n",
    "    dataframe, ['Load', 'Temperature'], \n",
    "    start_date=START_DATE, \n",
    "    timesteps=HISTORY_SIZE, \n",
    "    steps_ahead=STEPS_AHEAD,\n",
    "    window_days=WINDOW_SIZE_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data shape:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Validation/Testing data shape:')\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256       # batch size\n",
    "EPOCHS = 10            # number of epochs for training\n",
    "STEPS_PER_EPOCH = 100  # steps per epoch for training\n",
    "VAL_STEPS = 50         # validation steps during training\n",
    "WAIT = 50              # patience for early stopping\n",
    "LR = 1e-3              # learning rate\n",
    "BUFFER_SIZE = 10000    # buffer size for shuffling batches using tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays using tf.data.Dataset\n",
    "# Training dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# Cache data, shuffle using a buffer, create batches while droping \n",
    "# a remainder of the data and finally repeat indefinitely as needed\n",
    "# by the steps_per_epoch parameter for the number of epochs specified.\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
    "# Validation dataset\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_data = test_data.batch(BATCH_SIZE, drop_remainder=True).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional (LSTM) deep ANN using functional `tf.keras` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional two-layer deep ANN without regularization\n",
    "# tf.keras functional API\n",
    "input_layer = keras.layers.Input(shape=(X_train.shape[1:]), batch_size=BATCH_SIZE)\n",
    "x = keras.layers.LSTM(units=32, return_sequences=True, activation='relu')(input_layer)\n",
    "x = keras.layers.LSTM(units=32, activation='relu')(x)\n",
    "output_layer = keras.layers.Dense(STEPS_AHEAD)(x)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "adam = keras.optimizers.Adam(learning_rate=LR)\n",
    "model.compile(loss='mae', optimizer=adam, metrics=['mape'])\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, \n",
    "                                           patience=WAIT, verbose=1,\n",
    "                                           restore_best_weights=True)\n",
    "history = model.fit(train_data, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_data=test_data, validation_steps=VAL_STEPS,\n",
    "                    callbacks=[early_stop], shuffle=False, verbose=1,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "print('MAE val_loss at final epoch is {:.2f}, while min. val_loss is {:.2f}.'\n",
    "      .format(val_loss[-1], min(val_loss)))\n",
    "plt.plot(loss, label='training')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in test_data.take(3):\n",
    "    # Predict using model\n",
    "    y_hat = model.predict(X)[0]\n",
    "    # Convert back to the original scale\n",
    "    y_predicted = mean_std_values['Load'][0] + y_hat*mean_std_values['Load'][1]\n",
    "    y_true = mean_std_values['Load'][0] + y[0]*mean_std_values['Load'][1]\n",
    "    # MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "    print('MAPE = {:.2f} (%)'.format(mape))\n",
    "    # Plot predictions against true values\n",
    "    plt.plot(y_predicted, label='Prediction')\n",
    "    plt.plot(y_true, label='Actual')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
