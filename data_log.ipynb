{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electric Energy Load Demand Forecasting for Rural Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read load data \n",
    "def data_reader(file_name):\n",
    "    data = pd.read_excel(file_name, parse_dates=True, index_col='Time', usecols=range(2))\n",
    "    return data\n",
    "\n",
    "# function to read weather data\n",
    "def weather_reader(file_name):\n",
    "    weather = pd.read_excel(file_name, parse_dates=True, index_col='Time measured')\n",
    "    return weather\n",
    "\n",
    "# function for concatenating load data and weather data for training\n",
    "def concat_data(file_name_load, file_name_weather):\n",
    "    train_data = pd.concat([file_name_load, file_name_weather], axis=1)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather & load time-series data\n",
    "load_data = data_reader('Index_Bjønntjønn_2014_2018.xlsx')\n",
    "weather_data = weather_reader('bo_temp_2014_2018.xlsx')\n",
    "weather_data = weather_data.interpolate()\n",
    "\n",
    "# Concatenate\n",
    "dataframe = concat_data(load_data, weather_data)\n",
    "\n",
    "# Renaming columns for easier interpreting:\n",
    "dataframe = dataframe.rename(columns={\"Total\":\"Load\",\"Middeltemperatur i 2m høyde (TM)\": \"Temperature\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(data, time_start, time_end=None):\n",
    "    # Ploting time-series data with different time ranges\n",
    "    fig, ax = plt.subplots(figsize=(7,4.5))\n",
    "    ax2 = ax.twinx()\n",
    "    data['Load'].loc[time_start:time_end].plot(c='seagreen', label='Load', ax=ax)\n",
    "    if time_end is None:\n",
    "        data['Temperature'].loc[time_start].plot(c='darkorange', label='Temperature', ax=ax2)\n",
    "    else:\n",
    "        data['Temperature'].loc[time_start:time_end].plot(c='darkorange', label='Temperature', ax=ax2)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax.set_ylabel('Load', fontsize=12, fontweight='bold', color='seagreen')\n",
    "    ax2.set_ylabel('Temperature', fontsize=12, fontweight='bold', color='darkorange')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series for 2018\n",
    "show_plots(dataframe, '2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series for June to August 2018\n",
    "show_plots(dataframe, '2018-06', '2018-08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots(dataframe, '2018-06-01', '2018-06-07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coding holiday dates\n",
    "holiday_dates = {\n",
    "    # 2014 year\n",
    "    '2014-Jan-1st': ('2014-01-01', None),  # single day\n",
    "    '2014-Easter': ('2014-04-14', '2014-04-21'),  # date range\n",
    "    '2014-May-1st': ('2014-05-01', None),\n",
    "    '2014-Pentecost': ('2014-06-07', '2014-06-10'),\n",
    "    '2014-Xmas': ('2014-12-21', '2014-12-31'),\n",
    "    # 2015 year\n",
    "    '2015-Jan-1st': ('2015-01-01', None),\n",
    "    '2015-Easter': ('2015-03-30', '2015-04-06'),\n",
    "    '2015-May-1st': ('2015-05-01', None),  # Friday\n",
    "    '2015-Ascension': ('2015-05-14', None),\n",
    "    '2015-Pentecost': ('2014-05-24', '2014-05-25'),\n",
    "    '2015-Xmas': ('2015-12-23', '2015-12-31'),\n",
    "    # 2016 year\n",
    "    '2016-Jan-1st': ('2016-01-01', None),\n",
    "    '2016-Easter': ('2016-03-21', '2016-03-28'),\n",
    "    '2016-May-1st': ('2015-05-01', None),  # Sunday\n",
    "    '2016-Ascension': ('2016-05-05', None),\n",
    "    '2016-Pentecost': ('2016-05-16', '2016-05-17'),\n",
    "    '2016-Xmas': ('2016-12-26', '2016-12-31'),\n",
    "    # 2017 year\n",
    "    '2017-Jan-1st': ('2017-01-01', None),\n",
    "    '2017-Easter': ('2017-04-10', '2017-04-17'),\n",
    "    '2017-May-1st': ('2017-05-01', None),  # Monday\n",
    "    '2017-May-17th': ('2017-05-17', None),  # Wednesday\n",
    "    '2017-Ascension': ('2017-05-25', None),\n",
    "    '2017-Pentecost': ('2017-06-05', None),\n",
    "    '2017-Xmas': ('2017-12-25', '2017-12-31'),\n",
    "    # 2018 year\n",
    "    '2018-Jan-1st': ('2018-01-01', None),\n",
    "    '2018-Easter': ('2018-03-26', '2018-04-02'),\n",
    "    '2018-May-1st': ('2018-05-01', None),  # Tuesday\n",
    "    '2018-Ascension': ('2017-05-10', None),  # Thursday\n",
    "    '2018-May-17th': ('2017-05-17', None),\n",
    "    '2018-Pentecost': ('2018-05-21', None),\n",
    "    '2018-Xmas': ('2018-12-24', '2018-12-31')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(dataframe, holiday_dates, columns, time_lags=24, \n",
    "                      steps_ahead=1, drop_nan_rows=True):\n",
    "    \"\"\"Engineering features\n",
    "    \n",
    "    Load data features column names with underscore (i.e. Load_1h, Load_2h, etc.) \n",
    "    represent time-lags (t-1, t-2, ...), while those with plus sign (i.e. Load+1h, \n",
    "    Load+2h, etc.) represent future time-steps (t+1, t+2, ...); column with name\n",
    "    Load+0h represents current load at time instant t. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pandas dataframe\n",
    "        original dataframe with time-series data\n",
    "    holiday_dates: dictionary\n",
    "        dictionary with tuples specifying local holiday dates or date-ranges\n",
    "    columns: list\n",
    "        list of column names from the dataframe which are used for the \n",
    "        features engineering (i.e. time-lags)\n",
    "    time_lags: int\n",
    "        number of time lags for use with feature engineering\n",
    "    steps_ahead: int\n",
    "        number of steps ahead for multi-step forecasting (steps_ahead=1\n",
    "        means single-step ahead forecasting)\n",
    "    drop_nan_rows: bool\n",
    "        True/False indicator to drop rows with NaN values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe: pandas dataframe \n",
    "        dataframe augmented with additional features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the original dataframe\n",
    "    data = dataframe[columns].copy()\n",
    "            \n",
    "    # Features engineering\n",
    "    for col in data.columns:\n",
    "        for i in range(1, time_lags+1):\n",
    "            # Shift data by lag of 1 to time_lags (default: 24) hours\n",
    "            data[col+'_{:d}h'.format(i)] = data[col].shift(periods=i)  # time-lag\n",
    "        data[col+'_diff'] = data[col].diff()  # first-difference\n",
    "        data[col+'_week'] = data[col].shift(periods=24*7)  # previous week\n",
    "    \n",
    "    # Hour-of-day indicators with cyclical transform\n",
    "    dayhour_ind = data.index.hour\n",
    "    data['hr_sin'] = np.sin(dayhour_ind*(2.*np.pi/24))\n",
    "    data['hr_cos'] = np.cos(dayhour_ind*(2.*np.pi/24))\n",
    "    \n",
    "    # Day-of-week indicators with cyclical transform\n",
    "    weekday_ind = data.index.weekday\n",
    "    data['week_sin'] = np.sin(weekday_ind*(2.*np.pi/7))\n",
    "    data['week_cos'] = np.cos(weekday_ind*(2.*np.pi/7))\n",
    "\n",
    "    # Weekend as a binary indicator\n",
    "    data['weekend'] = np.asarray([0 if ind <= 4 else 1 for ind in weekday_ind])\n",
    "\n",
    "    # Month indicators with cyclical transform\n",
    "    month_ind = data.index.month\n",
    "    data['mnth_sin'] = np.sin((month_ind-1)*(2.*np.pi/12))\n",
    "    data['mnth_cos'] = np.cos((month_ind-1)*(2.*np.pi/12))\n",
    "    \n",
    "    # Holidays as a binary indicator\n",
    "    data['holidays'] = 0\n",
    "    for holiday, date in holiday_dates.items():\n",
    "        if date[1] is None:\n",
    "            # Single day\n",
    "            data.loc[date[0], 'holidays'] = 1\n",
    "        else:\n",
    "            # Date range\n",
    "            data.loc[date[0]:date[1], 'holidays'] = 1\n",
    "    \n",
    "    # Forecast horizont\n",
    "    if steps_ahead == 1:\n",
    "        # Single-step forecasting\n",
    "        data['Load+0h'] = data['Load'].values\n",
    "    else:\n",
    "        # Multi-step forecasting\n",
    "        for i in range(steps_ahead):\n",
    "            data['Load'+'+{:d}h'.format(i)] = data['Load'].shift(-i)\n",
    "    del data['Load']\n",
    "    \n",
    "    if drop_nan_rows:\n",
    "        # Drop rows with NaN values\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataframe, start_date, window_days=100, train_percent=80.,\n",
    "                     return_arrays=False):\n",
    "    \"\"\"Train and test data set split\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pandas dataframe\n",
    "        dataframe augmented with additional features\n",
    "    start_date: string\n",
    "        starting date of the time-series \n",
    "    window_days: int\n",
    "        size of the data window in days\n",
    "    train_percent: float\n",
    "        percentage of the data window size to use for creating the \n",
    "        training data set (the rest is used for testing)\n",
    "    return_arrays: bool\n",
    "        True/False indicator which defines the type of output; if \n",
    "        True function returns numpy arrays; if False it returns\n",
    "        pandas dataframes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train: dataframe or array\n",
    "        training data 2D array of input features\n",
    "    y_train: dataframe or array\n",
    "        training data array of output values\n",
    "    X_test: dataframe or array\n",
    "        testing/validation data 2D array of input features\n",
    "    y_test: dataframe or array\n",
    "        testing/validation data array of output values\n",
    "    \"\"\"\n",
    "    data = dataframe.copy()\n",
    "    if window_days*24 > data.values.shape[0]:\n",
    "        raise ValueError('Variable window_days has too large value: {}*24h = {} > {}, \\\n",
    "            which is more than there is data!'.format(window_days, window_days*24, \n",
    "                                                      data.values.shape[0]))\n",
    "    \n",
    "    # Split dataframe into X, y\n",
    "    columns = data.columns.values\n",
    "    outputs = [col_name for col_name in columns if 'Load+' in col_name]\n",
    "    inputs = [col_name for col_name in columns if col_name not in outputs]\n",
    "    # inputs (features)\n",
    "    X = data[inputs]\n",
    "    # outputs\n",
    "    y = data[outputs]\n",
    "    \n",
    "    # Training period\n",
    "    train_percent = train_percent/100.\n",
    "    st = pd.to_datetime(start_date)  # start date\n",
    "    et = st + dt.timedelta(days=int(train_percent*window_days))  # end date\n",
    "    X_train = X.loc[st:et]\n",
    "    y_train = y.loc[st:et]\n",
    "    \n",
    "    # Testing / Validation period\n",
    "    sv = et \n",
    "    ev = sv + dt.timedelta(days=int((1-train_percent)*window_days)+1)\n",
    "    X_test = X.loc[sv:ev]\n",
    "    y_test = y.loc[sv:ev]\n",
    "        \n",
    "    if return_arrays:\n",
    "        # Returning numpy arrays\n",
    "        return X_train.values, y_train.values, X_test.values, y_test.values\n",
    "    else:\n",
    "        # Returning pandas dataframes\n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_AHEAD = 24\n",
    "# 24-hours ahead forecasting (steps_ahead=24)\n",
    "data_features = engineer_features(dataframe, holiday_dates, \n",
    "                                  columns=['Load', 'Temperature'], \n",
    "                                  steps_ahead=STEPS_AHEAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2014-01-09'\n",
    "WINDOW_SIZE_DAYS = 400\n",
    "# Split dataset into training and test/validation sets\n",
    "X_train, y_train, X_test, y_test = train_test_split(data_features, start_date=START_DATE, \n",
    "                                                    window_days=WINDOW_SIZE_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and transform input data\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 400\n",
    "WAIT = 50  # patience\n",
    "LR = 1e-3  # learning rate\n",
    "# Feed-forward and fixed funnel-shaped deep ANN\n",
    "# tf.keras functional API\n",
    "input_layer = keras.layers.Input(shape=X_train_.shape[1:])\n",
    "x = keras.layers.Dense(units=1024, activation='relu')(input_layer)\n",
    "x = keras.layers.Dropout(0.2)(x)  # regularization\n",
    "x = keras.layers.Dense(units=512, activation='relu')(x)\n",
    "x = keras.layers.Dense(units=512, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.1)(x)  # regularization\n",
    "x = keras.layers.Dense(units=256, activation='relu')(x)\n",
    "output_layer = keras.layers.Dense(STEPS_AHEAD)(x)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "adam = keras.optimizers.Adam(learning_rate=LR, decay=LR/EPOCHS)\n",
    "model.compile(loss='mae', optimizer=adam, metrics=['mae', 'mape'])\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, \n",
    "                                           patience=WAIT, verbose=1,\n",
    "                                           restore_best_weights=True)\n",
    "history = model.fit(X_train_sc, y_train.values, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    validation_data=(X_test_sc, y_test.values),\n",
    "                    callbacks=[early_stop], shuffle=True, verbose=0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "print('MAE val_loss at final epoch is {:.2f}, while min. val_loss is {:.2f}.'\n",
    "      .format(val_loss[-1], min(val_loss)))\n",
    "plt.plot(loss, label='train')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(dataframe, start_date, window_days, test_size=1):\n",
    "    \"\"\" Prepare test data\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe: pandas dataframe\n",
    "        original dataframe with features\n",
    "    start_date: string\n",
    "        starting date for the time-series previously used\n",
    "        in creating the train and test/validation data sets\n",
    "    window_days: int\n",
    "        size of the data window in days previously used\n",
    "        in creating the train and test/validation data sets\n",
    "    test_size: int\n",
    "        number of time-steps (hours) for walk-forward testing\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_new: numpy array\n",
    "        walk-forward testing data set as numpy array\n",
    "    \"\"\"\n",
    "    data = dataframe.copy()\n",
    "    date_test_start = pd.to_datetime(start_date) + dt.timedelta(days=window_days)\n",
    "    date_test_end = date_test_start + dt.timedelta(hours=test_size)\n",
    "    \n",
    "    columns = data.columns.values\n",
    "    outputs = [col_name for col_name in columns if 'Load+' in col_name]\n",
    "    inputs = [col_name for col_name in columns if col_name not in outputs]\n",
    "    \n",
    "    if test_size == 1:\n",
    "        X_new = data[inputs].loc[date_test_start].values.reshape(1,-1)\n",
    "    else:\n",
    "        X_new = data[inputs].loc[date_test_start:date_test_end].values[:-1]\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 12  # walk-forward for 12 hours\n",
    "X_new = prepare_test_data(data_features, START_DATE, WINDOW_SIZE_DAYS, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_sc = scaler.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new values\n",
    "y_pred = model.predict(X_new_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred)/y_true))*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multi-step ahead predictions using the walk-forward method\n",
    "if TEST_SIZE == 1: \n",
    "    raise ValueError('TEST_SIZE: Need a multi-step ahead predictions!')\n",
    "date_start = pd.to_datetime(START_DATE) + dt.timedelta(days=WINDOW_SIZE_DAYS)\n",
    "for i in range(TEST_SIZE):\n",
    "    date_end = date_start + dt.timedelta(hours=23)\n",
    "    y_true = data_features['Load+0h'].loc[date_start:date_end]\n",
    "    y_values = pd.DataFrame(y_true)\n",
    "    y_values = y_values.rename(columns={'Load+0h':'Actual'})\n",
    "    y_values['Predicted'] = y_pred[i,:]\n",
    "    \n",
    "    # Absolute percentage error\n",
    "    y_values['APE'] = np.abs((y_values['Actual'] - y_values['Predicted'])/y_values['Actual'])*100.\n",
    "    # Mean absolute percentage error\n",
    "    mape = mean_absolute_percentage_error(y_values['Actual'].values, y_values['Predicted'].values)\n",
    "    print('MAPE = {:.2f} (%)'.format(mape))\n",
    "    \n",
    "    # Plot figure\n",
    "    y_values[['Actual', 'Predicted']].plot(figsize=(5.5,3.5))\n",
    "    plt.ylabel('Load')\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    date_start = date_start + dt.timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
