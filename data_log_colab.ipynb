{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NANG6WuTdsZy"
   },
   "source": [
    "## Electric Energy Load Demand Forecasting for Rural Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor, RegressorChain\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BISlpoMMvipB"
   },
   "outputs": [],
   "source": [
    "# Run this cell only in the Google Colab\n",
    "# Tensorflow version 2.x\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ONLY in the Google Colab\n",
    "! pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ONLY in the Google Colab\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qNMoUfmMoCn4"
   },
   "outputs": [],
   "source": [
    "# Test for GPU presence\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found!')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHs-Bcmtvtph"
   },
   "outputs": [],
   "source": [
    "# Run this cell only in the Google Colab\n",
    "# Display GPU information\n",
    "!nvidia-smi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    # Seaborn style (figure aesthetics only)\n",
    "    sns.set(context='paper', style='whitegrid', font_scale=1.2)\n",
    "    sns.set_style('ticks', {'xtick.direction':'in', 'ytick.direction':'in'})\n",
    "except ImportError:\n",
    "    print('Seaborn not installed. Going without it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYNRrVz0dsaE"
   },
   "source": [
    "### Load and Weather time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read load data \n",
    "def data_reader(file_name):\n",
    "    data = pd.read_excel(file_name, parse_dates=True, \n",
    "                            index_col='Time', usecols=range(2))\n",
    "    return data\n",
    "\n",
    "\n",
    "# function to read weather data\n",
    "def weather_reader(file_name):\n",
    "    weather = pd.read_excel(file_name, parse_dates=True, \n",
    "                            index_col='Time measured')\n",
    "    return weather\n",
    "\n",
    "\n",
    "def data_reader_colab(filename, colab_files):\n",
    "    \"\"\" Read data inside the Google Colab\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    file_name: string\n",
    "        name of the excel file which is being read\n",
    "    colab_files: dict\n",
    "        dictionary returned by the google.colab with keys \n",
    "        as file names and values as binary files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: pd.DataFrame\n",
    "        pandas dataframe holding time-series load data\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(io.BytesIO(colab_files[filename]), \n",
    "                         parse_dates=True, index_col='Time', \n",
    "                         usecols=range(2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def weather_reader_colab(filename, colab_files):\n",
    "    \"\"\" Read data inside the Google Colab\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    file_name: string\n",
    "        name of the excel file which is being read\n",
    "    colab_files: dict\n",
    "        dictionary returned by the google.colab with keys \n",
    "        as file names and values as binary files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: pd.DataFrame\n",
    "        pandas dataframe holding time-series weather data\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(io.BytesIO(colab_files[filename]), \n",
    "                         parse_dates=True, \n",
    "                         index_col='Time measured')\n",
    "    return data\n",
    "\n",
    "\n",
    "# function for concatenating load data and weather data for training\n",
    "def concat_data(file_name_load, file_name_weather):\n",
    "    train_data = pd.concat([file_name_load, file_name_weather], axis=1)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather & load time-series data\n",
    "load_file_name = 'Index_Bjønntjønn_2014_2018.xlsx'\n",
    "weather_file_name = 'bo_temp_2014_2018.xlsx'\n",
    "\n",
    "# If the files have already been uploaded there is no need to do it again.\n",
    "file_names = [load_file_name, weather_file_name]\n",
    "files_present = 0\n",
    "for file_name in file_names:\n",
    "    try:\n",
    "        f = open(file_name, 'rb')\n",
    "        print('Found file with a file name: {}'.format(file_name))\n",
    "        files_present += 1\n",
    "    except (IOError, FileNotFoundError):\n",
    "        print('There is NO file with a file name: {}'.format(file_name))\n",
    "    else:\n",
    "        f.close()\n",
    "# Check if all files are already present\n",
    "if files_present == 2:\n",
    "    # Don't need to upload files (they already exist!).\n",
    "    UPLOAD = False\n",
    "    print('All files are already uploaded.')\n",
    "else:\n",
    "    # Need to upload files (they don't exist!).\n",
    "    UPLOAD = True\n",
    "    print('Need to upload files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab: upload files if there is none already present.\n",
    "try:\n",
    "    from google.colab import files\n",
    "    # We are inside the Google Colab\n",
    "    COLAB = True\n",
    "    # If the files have already been uploaded there is no need to do it again.\n",
    "    if UPLOAD == True:\n",
    "        # Google Colab file upload (upload files only once)\n",
    "        uploaded_files = files.upload()\n",
    "except ImportError:\n",
    "    # We are NOT inside the Google Colab\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB and UPLOAD:\n",
    "    # Reading files just uploaded inside a Google Colab\n",
    "    load_data = data_reader_colab(load_file_name, uploaded_files)\n",
    "    weather_data = weather_reader_colab(weather_file_name, uploaded_files)\n",
    "else:\n",
    "    # Reading existing files from a local disk or a virtual storage\n",
    "    load_data = data_reader(load_file_name)\n",
    "    weather_data = weather_reader(weather_file_name)\n",
    "\n",
    "# Interpolate weather data time-series\n",
    "weather_data = weather_data.interpolate()\n",
    "# Concatenate\n",
    "dataframe = concat_data(load_data, weather_data)\n",
    "# Rename columns for easier interpreting:\n",
    "dataframe = dataframe.rename(columns={\"Total\":\"Load\",\n",
    "                                      \"Middeltemperatur i 2m høyde (TM)\": \"Temperature\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrIhH0iydsaQ"
   },
   "outputs": [],
   "source": [
    "def show_plots(data, time_start, time_end=None):\n",
    "    # Ploting time-series data with different time ranges\n",
    "    fig, ax = plt.subplots(figsize=(7,4.5))\n",
    "    ax2 = ax.twinx()\n",
    "    if time_end is None:\n",
    "        data['Load'].loc[time_start].plot(\n",
    "            c='seagreen', label='Load', ax=ax)\n",
    "        data['Temperature'].loc[time_start].plot(\n",
    "            c='darkorange', label='Temperature', ax=ax2)\n",
    "    else:\n",
    "        data['Load'].loc[time_start:time_end].plot(\n",
    "            c='seagreen', label='Load', ax=ax)\n",
    "        data['Temperature'].loc[time_start:time_end].plot(\n",
    "            c='darkorange', label='Temperature', ax=ax2)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax.set_ylabel('Load', fontsize=12, fontweight='bold', \n",
    "                  color='seagreen')\n",
    "    ax2.set_ylabel('Temperature', fontsize=12, \n",
    "                   fontweight='bold', color='darkorange')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16Gjk6IedsaU"
   },
   "outputs": [],
   "source": [
    "# Time-series for 2018\n",
    "show_plots(dataframe, '2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ktl5aFzVdsaZ"
   },
   "outputs": [],
   "source": [
    "# Time-series for June to August 2018\n",
    "show_plots(dataframe, '2018-06', '2018-08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fysYZuUDdsac"
   },
   "outputs": [],
   "source": [
    "show_plots(dataframe, '2018-06-01', '2018-06-07')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtINiijXdsaf"
   },
   "source": [
    "### Multivariate multi-step load forecasting with feed-forward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coding holiday dates\n",
    "holiday_dates = {\n",
    "    # 2014 year\n",
    "    '2014-Jan-1st': ('2014-01-01', None),  # single day\n",
    "    '2014-Easter': ('2014-04-14', '2014-04-21'),  # date range\n",
    "    '2014-May-1st': ('2014-05-01', None),\n",
    "    '2014-Pentecost': ('2014-06-07', '2014-06-10'),\n",
    "    '2014-Xmas': ('2014-12-21', '2014-12-31'),\n",
    "    # 2015 year\n",
    "    '2015-Jan-1st': ('2015-01-01', None),\n",
    "    '2015-Easter': ('2015-03-30', '2015-04-06'),\n",
    "    '2015-May-1st': ('2015-05-01', None),  # Friday\n",
    "    '2015-Ascension': ('2015-05-14', None),\n",
    "    '2015-Pentecost': ('2014-05-24', '2014-05-25'),\n",
    "    '2015-Xmas': ('2015-12-23', '2015-12-31'),\n",
    "    # 2016 year\n",
    "    '2016-Jan-1st': ('2016-01-01', None),\n",
    "    '2016-Easter': ('2016-03-21', '2016-03-28'),\n",
    "    '2016-May-1st': ('2015-05-01', None),  # Sunday\n",
    "    '2016-Ascension': ('2016-05-05', None),\n",
    "    '2016-Pentecost': ('2016-05-16', '2016-05-17'),\n",
    "    '2016-Xmas': ('2016-12-26', '2016-12-31'),\n",
    "    # 2017 year\n",
    "    '2017-Jan-1st': ('2017-01-01', None),\n",
    "    '2017-Easter': ('2017-04-10', '2017-04-17'),\n",
    "    '2017-May-1st': ('2017-05-01', None),  # Monday\n",
    "    '2017-May-17th': ('2017-05-17', None),  # Wednesday\n",
    "    '2017-Ascension': ('2017-05-25', None),\n",
    "    '2017-Pentecost': ('2017-06-05', None),\n",
    "    '2017-Xmas': ('2017-12-25', '2017-12-31'),\n",
    "    # 2018 year\n",
    "    '2018-Jan-1st': ('2018-01-01', None),\n",
    "    '2018-Easter': ('2018-03-26', '2018-04-02'),\n",
    "    '2018-May-1st': ('2018-05-01', None),  # Tuesday\n",
    "    '2018-Ascension': ('2017-05-10', None),  # Thursday\n",
    "    '2018-May-17th': ('2017-05-17', None),\n",
    "    '2018-Pentecost': ('2018-05-21', None),\n",
    "    '2018-Xmas': ('2018-12-24', '2018-12-31')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(dataframe, holiday_dates, columns, time_lags=24, \n",
    "                      steps_ahead=1, drop_nan_rows=True):\n",
    "    \"\"\"Engineering features\n",
    "    \n",
    "    Load data features column names with underscore (i.e. Load_1h, Load_2h, etc.) \n",
    "    represent time-lags (t-1, t-2, ...), while those with plus sign (i.e. Load+1h, \n",
    "    Load+2h, etc.) represent future time-steps (t+1, t+2, ...); column with name\n",
    "    Load+0h represents current load at time instant t. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pd.DataFrame\n",
    "        original dataframe with time-series data\n",
    "    holiday_dates: dictionary\n",
    "        dictionary with tuples specifying local holiday dates or date-ranges\n",
    "    columns: list\n",
    "        list of column names from the dataframe which are used for the \n",
    "        features engineering (i.e. time-lags)\n",
    "    time_lags: int\n",
    "        number of time lags for use with feature engineering\n",
    "    steps_ahead: int\n",
    "        number of steps ahead for multi-step forecasting (steps_ahead=1\n",
    "        means single-step ahead forecasting)\n",
    "    drop_nan_rows: bool\n",
    "        True/False indicator to drop rows with NaN values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe: pd.DataFrame\n",
    "        dataframe augmented with additional features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the original dataframe\n",
    "    data = dataframe[columns].copy()\n",
    "            \n",
    "    # Features engineering\n",
    "    for col in data.columns:\n",
    "        for i in range(1, time_lags+1):\n",
    "            # Shift data by lag of 1 to time_lags (default: 24) hours\n",
    "            data[col+'_{:d}h'.format(i)] = data[col].shift(periods=i)  # time-lag\n",
    "        data[col+'_diff'] = data[col].diff()  # first-difference\n",
    "        data[col+'_week'] = data[col].shift(periods=24*7)  # previous week\n",
    "    \n",
    "    # Hour-of-day indicators with cyclical transform\n",
    "    dayhour_ind = data.index.hour\n",
    "    data['hr_sin'] = np.sin(dayhour_ind*(2.*np.pi/24))\n",
    "    data['hr_cos'] = np.cos(dayhour_ind*(2.*np.pi/24))\n",
    "    \n",
    "    # Day-of-week indicators with cyclical transform\n",
    "    weekday_ind = data.index.weekday\n",
    "    data['week_sin'] = np.sin(weekday_ind*(2.*np.pi/7))\n",
    "    data['week_cos'] = np.cos(weekday_ind*(2.*np.pi/7))\n",
    "\n",
    "    # Weekend as a binary indicator\n",
    "    data['weekend'] = np.asarray([0 if ind <= 4 else 1 for ind in weekday_ind])\n",
    "\n",
    "    # Month indicators with cyclical transform\n",
    "    month_ind = data.index.month\n",
    "    data['mnth_sin'] = np.sin((month_ind-1)*(2.*np.pi/12))\n",
    "    data['mnth_cos'] = np.cos((month_ind-1)*(2.*np.pi/12))\n",
    "    \n",
    "    # Holidays as a binary indicator\n",
    "    data['holidays'] = 0\n",
    "    for holiday, date in holiday_dates.items():\n",
    "        if date[1] is None:\n",
    "            # Single day\n",
    "            data.loc[date[0], 'holidays'] = 1\n",
    "        else:\n",
    "            # Date range\n",
    "            data.loc[date[0]:date[1], 'holidays'] = 1\n",
    "    \n",
    "    # Forecast horizont\n",
    "    if steps_ahead == 1:\n",
    "        # Single-step forecasting\n",
    "        data['Load+0h'] = data['Load'].values\n",
    "    else:\n",
    "        # Multi-step forecasting\n",
    "        for i in range(steps_ahead):\n",
    "            data['Load'+'+{:d}h'.format(i)] = data['Load'].shift(-i)\n",
    "    del data['Load']\n",
    "    \n",
    "    if drop_nan_rows:\n",
    "        # Drop rows with NaN values\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataframe, start_date, window_days=100, train_percent=80.,\n",
    "                     return_arrays=False):\n",
    "    \"\"\"Train and test data set split\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pd.DataFrame\n",
    "        dataframe augmented with additional features\n",
    "    start_date: string\n",
    "        starting date of the time-series \n",
    "    window_days: int\n",
    "        size of the data window in days\n",
    "    train_percent: float\n",
    "        percentage of the data window size to use for creating the \n",
    "        training data set (the rest is used for testing)\n",
    "    return_arrays: bool\n",
    "        True/False indicator which defines the type of output; if \n",
    "        True function returns numpy arrays; if False it returns\n",
    "        pandas dataframes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train: pd.DataFrame or np.array\n",
    "        training data 2D array of input features\n",
    "    y_train: pd.DataFrame or np.array\n",
    "        training data array of target values\n",
    "    X_test: pd.DataFrame or np.array\n",
    "        testing/validation data 2D array of input features\n",
    "    y_test: pd.DataFrame or np.array\n",
    "        testing/validation data array of target values\n",
    "    \"\"\"\n",
    "    data = dataframe.copy()\n",
    "    if window_days*24 > data.values.shape[0]:\n",
    "        raise ValueError('Variable window_days has too large value: {}*24h = {} > {}, \\\n",
    "            which is more than there is data!'.format(window_days, window_days*24, \n",
    "                                                      data.values.shape[0]))\n",
    "    \n",
    "    # Split dataframe into X, y\n",
    "    columns = data.columns.values\n",
    "    outputs = [col_name for col_name in columns if 'Load+' in col_name]\n",
    "    inputs = [col_name for col_name in columns if col_name not in outputs]\n",
    "    # inputs (features)\n",
    "    X = data[inputs]\n",
    "    # outputs\n",
    "    y = data[outputs]\n",
    "    \n",
    "    # Training period\n",
    "    train_percent = train_percent/100.\n",
    "    st = pd.to_datetime(start_date)  # start date\n",
    "    et = st + dt.timedelta(days=int(train_percent*window_days))  # end date\n",
    "    X_train = X.loc[st:et]\n",
    "    y_train = y.loc[st:et]\n",
    "    \n",
    "    # Testing / Validation period\n",
    "    sv = et \n",
    "    ev = sv + dt.timedelta(days=int((1-train_percent)*window_days)+1)\n",
    "    X_test = X.loc[sv:ev]\n",
    "    y_test = y.loc[sv:ev]\n",
    "        \n",
    "    if return_arrays:\n",
    "        # Returning numpy arrays\n",
    "        return X_train.values, y_train.values, X_test.values, y_test.values\n",
    "    else:\n",
    "        # Returning pandas dataframes\n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day-ahead short-term load forecasting with time-series data\n",
    "STEPS_AHEAD = 24           # hours-ahead for prediction\n",
    "START_DATE = '2014-01-09'  # starting date of the time-series\n",
    "WINDOW_SIZE_DAYS = 400     # window size in days for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day-ahead (i.e. 24-hours ahead) forecasting\n",
    "data_features = engineer_features(dataframe, holiday_dates, \n",
    "                                  columns=['Load', 'Temperature'], \n",
    "                                  steps_ahead=STEPS_AHEAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test/validation sets\n",
    "X_train, y_train, X_test, y_test = train_test_split(\n",
    "    data_features, \n",
    "    start_date=START_DATE, \n",
    "    window_days=WINDOW_SIZE_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and transform input data\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine multi-step regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-step ahead time-series regression\n",
    "# Randomized grid search with cross-validation\n",
    "parameters = [{'C':stats.uniform(loc=1e-4, scale=1e5),\n",
    "              'epsilon':stats.uniform(loc=1., scale=1e4),\n",
    "              'gamma':['auto', 'scale']}]\n",
    "svr = RandomizedSearchCV(estimator=SVR(kernel='rbf'), \n",
    "                         param_distributions=parameters, n_iter=5000, \n",
    "                         cv=TimeSeriesSplit(n_splits=3), \n",
    "                         scoring='neg_mean_squared_error', \n",
    "                         refit=True, n_jobs=-1, verbose=1)\n",
    "svr.fit(X_train, y_train.values[:,0])  # do not use scaled data\n",
    "print(svr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using best hyper-parameters from the single-step ahead regression\n",
    "multi_svr = MultiOutputRegressor(estimator=SVR(kernel='rbf', **svr.best_params_), n_jobs=-1)\n",
    "multi_svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A multi-step model that arranges regressions into a chain. Each model makes a prediction\n",
    "# in the order specified by the chain using all of the available features provided to the\n",
    "# model plus the predictions of models that are earlier in the chain. Base model is SVM!\n",
    "chain_svr = RegressorChain(base_estimator=SVR(kernel='rbf', **svr.best_params_))\n",
    "chain_svr.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree multi-step regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor supports multi-step output out-of-the-box!\n",
    "# Grid search with cross-validation\n",
    "parameters = [{'criterion':['mse', 'mae'],\n",
    "              'max_depth':[1, 5, None],\n",
    "              'max_features':['auto', 'log2', 0.5],\n",
    "              'max_leaf_nodes':[1, 2, None]}]\n",
    "tree = GridSearchCV(estimator=DecisionTreeRegressor(), \n",
    "                          param_grid=parameters,\n",
    "                          cv=TimeSeriesSplit(n_splits=3), \n",
    "                          scoring='neg_mean_squared_error', \n",
    "                          refit=True, n_jobs=-1, verbose=1)\n",
    "tree.fit(X_train, y_train)  # do not use scaled data\n",
    "print(tree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqT8f7Hgdsa9"
   },
   "source": [
    "## Feed-forward deep ANN using functional `tf.keras` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256  # batch size for training\n",
    "EPOCHS = 300      # epochs for training\n",
    "WAIT = 20         # patience for early stopping\n",
    "LR = 1e-3         # initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward and fixed funnel-shaped deep ANN\n",
    "# tf.keras functional API\n",
    "input_layer = keras.layers.Input(shape=X_train_sc.shape[1:])\n",
    "x = keras.layers.Dense(units=1024, activation='relu')(input_layer)\n",
    "x = keras.layers.Dropout(0.1)(x)  # regularization\n",
    "x = keras.layers.Dense(units=512, activation='relu')(x)\n",
    "x = keras.layers.Dense(units=512, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.1)(x)  # regularization\n",
    "x = keras.layers.Dense(units=256, activation='relu')(x)\n",
    "output_layer = keras.layers.Dense(STEPS_AHEAD)(x)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    LR, decay_steps=EPOCHS, decay_rate=0.96, staircase=False)\n",
    "adam = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(loss='mae', optimizer=adam, metrics=['mae', 'mape'])\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.1, patience=WAIT, \n",
    "    verbose=1, restore_best_weights=True)\n",
    "history = model.fit(X_train_sc, y_train.values, \n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    validation_split=0.2, shuffle=True, \n",
    "                    callbacks=[early_stop], verbose=1,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "print('MAE val_loss at final epoch is {:.2f}, while min. val_loss is {:.2f}.'\n",
    "      .format(val_loss[-1], min(val_loss)))\n",
    "plt.plot(loss, label='train')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `keras-tuner` for fine-tuning ANN architecture and hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(hp):\n",
    "    # Input layer\n",
    "    input_layer = keras.layers.Input(shape=X_train_sc.shape[1:])\n",
    "    x = input_layer\n",
    "    # Try different number of hidden layers\n",
    "    for i in range(hp.Int('num_layers', 2, 6)):\n",
    "        # Dense layer type\n",
    "        x = keras.layers.Dense(units=hp.Int('units_{}'.format(i), min_value=256, \n",
    "                                            max_value=1024, step=256), \n",
    "                               activation=hp.Choice('act_{}'.format(i), \n",
    "                                                    values=['relu', 'tanh']))(x)\n",
    "        # Regularization layer type\n",
    "        x = keras.layers.Dropout(hp.Float('drop_{}'.format(i), 0., 0.5, step=0.1, \n",
    "                                          default=0.1))(x)\n",
    "    # Output layer\n",
    "    output_layer = keras.layers.Dense(STEPS_AHEAD)(x)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    # Optimize learning rate as well\n",
    "    adam = keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ONLY in the Google Colab\n",
    "# Define keras tuner\n",
    "tuner = kt.Hyperband(build_keras_model, objective='val_loss', max_epochs=EPOCHS)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, \n",
    "                                           patience=WAIT, verbose=0,\n",
    "                                           restore_best_weights=True)\n",
    "tuner.search(X_train_sc, y_train.values, epochs=EPOCHS,\n",
    "             validation_data=(X_test_sc, y_test.values),\n",
    "             callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ONLY in the Google Colab\n",
    "# Summary of keras-tuner search results\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ONLY in the Google Colab\n",
    "# Re-create best model found by the keras-tuner\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "# View the summary of the best model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHvSU7kVdsbE"
   },
   "outputs": [],
   "source": [
    "# Save a trained model (architecture and weights) on disk.\n",
    "# This model can be loaded and used without re-training:\n",
    "# model = keras.models.load_model('model-dense-dnn.tf')\n",
    "# for making predictions, or it can be restored and tra-\n",
    "# ining of the model can be continued from where it was.\n",
    "model.save('model-dense-dnn.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(dataframe, start_date, window_days, test_size=1):\n",
    "    \"\"\" Prepare test data\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe: pd.DataFrame\n",
    "        original dataframe with features\n",
    "    start_date: string\n",
    "        starting date for the time-series previously used\n",
    "        in creating the train and test/validation data sets\n",
    "    window_days: int\n",
    "        size of the data window in days previously used\n",
    "        in creating the train and test/validation data sets\n",
    "    test_size: int\n",
    "        number of time-steps (hours) for walk-forward testing\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_new: np.array\n",
    "        walk-forward testing data set as numpy array\n",
    "    \"\"\"\n",
    "    data = dataframe.copy()\n",
    "    date_test_start = pd.to_datetime(start_date) + dt.timedelta(days=window_days)\n",
    "    date_test_end = date_test_start + dt.timedelta(hours=test_size)\n",
    "    \n",
    "    columns = data.columns.values\n",
    "    outputs = [col_name for col_name in columns if 'Load+' in col_name]\n",
    "    inputs = [col_name for col_name in columns if col_name not in outputs]\n",
    "    \n",
    "    if test_size == 1:\n",
    "        # Array with a single value needs to be reshaped accordingly\n",
    "        X_new = data[inputs].loc[date_test_start].values.reshape(1,-1)\n",
    "    else:\n",
    "        X_new = data[inputs].loc[date_test_start:date_test_end].values[:-1]\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 12  # walk-forward for 12 hours\n",
    "X_new = prepare_test_data(data_features, START_DATE, WINDOW_SIZE_DAYS, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test values using scaler\n",
    "X_new_sc = scaler.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new values \n",
    "# ---------------------\n",
    "# Feed-forward ANN\n",
    "y_pred_ann = model.predict(X_new_sc)\n",
    "# Support Vector Machine (Regressor)\n",
    "y_pred_svr_reg = multi_svr.predict(X_new)\n",
    "# Support Vector Machine (Chained)\n",
    "y_pred_svr_chn = chain_svr.predict(X_new)\n",
    "# DecisionTrees\n",
    "y_pred_tree = tree.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    \"\"\"Mean absolute percentage error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.array\n",
    "        array holding true values\n",
    "    y_pred: np.array\n",
    "        array holding predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mape: float\n",
    "        mean absolute percentage error value\n",
    "    \"\"\"\n",
    "    if len(y_true.shape) > 1 or len(y_pred.shape) > 1:\n",
    "        raise TypeError('Arrays need to be one-dimensional.')\n",
    "        \n",
    "    mape = np.mean(np.abs((y_true - y_pred)/y_true))*100.\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multi-step ahead predictions using the walk-forward method\n",
    "if TEST_SIZE == 1: \n",
    "    raise ValueError('TEST_SIZE: Need a multi-step ahead predictions!')\n",
    "    \n",
    "date_start = pd.to_datetime(START_DATE) + dt.timedelta(days=WINDOW_SIZE_DAYS)\n",
    "for i in range(TEST_SIZE):\n",
    "    date_end = date_start + dt.timedelta(hours=23)\n",
    "    y_true = data_features['Load+0h'].loc[date_start:date_end]\n",
    "    y_values = pd.DataFrame(y_true)\n",
    "    y_values = y_values.rename(columns={'Load+0h':'Actual'})\n",
    "    y_values['ANN'] = y_pred_ann[i,:]\n",
    "    y_values['SVR-Reg'] = y_pred_svr_reg[i,:]\n",
    "    y_values['SVR-Chn'] = y_pred_svr_chn[i,:]\n",
    "    y_values['Tree'] = y_pred_tree[i,:]\n",
    "    \n",
    "    # Ensemble predictions using weighted-average\n",
    "    models = ['ANN', 'SVR-Reg', 'SVR-Chn', 'Tree']\n",
    "    y_values['Ensemble'] = np.average(y_values[models].values, \n",
    "                                      axis=1,  # by columns\n",
    "                                      weights=[0.3, 0.25, 0.4, 0.05])  # with weights\n",
    "    \n",
    "    # Absolute percentage errors\n",
    "    y_values['APE-ANN'] = np.abs((y_values['Actual'] - y_values['ANN'])/y_values['Actual'])*100.\n",
    "    y_values['APE-SVR-Reg'] = np.abs((y_values['Actual'] - y_values['SVR-Reg'])/y_values['Actual'])*100.\n",
    "    y_values['APE-SVR-Chn'] = np.abs((y_values['Actual'] - y_values['SVR-Chn'])/y_values['Actual'])*100.\n",
    "    y_values['APE-Tree'] = np.abs((y_values['Actual'] - y_values['Tree'])/y_values['Actual'])*100.\n",
    "    y_values['APE-Ensemble'] = np.abs((y_values['Actual'] - y_values['Ensemble'])/y_values['Actual'])*100.\n",
    "\n",
    "    # Mean absolute percentage errors\n",
    "    for m in models:\n",
    "        mape = mean_absolute_percentage_error(y_values['Actual'].values, y_values[m].values)\n",
    "        print('MAPE-{} = {:.2f} (%)'.format(m, mape))\n",
    "    # Ensemble MAPE (weighted average of base models)\n",
    "    mape = mean_absolute_percentage_error(y_values['Actual'].values, \n",
    "                                          y_values['Ensemble'].values)\n",
    "    print('MAPE-Ensemble = {:.2f} (%)'.format(mape))\n",
    "    \n",
    "    # Plot figures\n",
    "    y_values[['Actual', 'ANN', 'SVR-Reg', 'SVR-Chn', 'Tree', 'Ensemble']].plot(figsize=(5.5,4))\n",
    "    plt.ylabel('Load')\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    date_start = date_start + dt.timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJg3qAdVdsbZ"
   },
   "source": [
    "# Multivariate multi-step load forecasting with convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQAcPL-Gdsba"
   },
   "outputs": [],
   "source": [
    "def samples_timesteps_features(dataframe, columns, start_date, timesteps=72, \n",
    "                               steps_ahead=24, window_days=100, train_percent=80.):\n",
    "    \"\"\"\n",
    "    Restructure original dataframe with columns time-series data into a 3D array\n",
    "    of shape: [samples, timesteps, features] for the use with convolutional layers.\n",
    "    First dimension of the output array is the number of samples, which is determined\n",
    "    automatically from the size of the window. Second dimension is determined by the \n",
    "    number of timesteps and defines how many time steps from the past will be used in\n",
    "    the internal processing of the convolutional layer. The third dimension is the\n",
    "    number of features in the original dataset, which is defined by the number of \n",
    "    columns that are used from the original dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pd.DataFrame\n",
    "        dataframe with the original time-series data\n",
    "    columns: list\n",
    "        list of column names from the dataframe which are used\n",
    "    start_date: string\n",
    "        starting date of the time-series \n",
    "    timesteps: int\n",
    "        number of time steps from the past for creating output arrays\n",
    "    steps_ahead: int\n",
    "        number of time steps into the future for making predictions\n",
    "    window_days: int\n",
    "        size of the data window in days\n",
    "    train_percent: float\n",
    "        percentage of the data window size to use for creating the \n",
    "        training data set (the rest is used for testing)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean_std_values: dictionary\n",
    "        dictionary with tuples holding mean value and standard\n",
    "        deviation for each of the columns in the dataframe\n",
    "    X_train: np.array\n",
    "        training data 2D array of features\n",
    "    y_train: np.array\n",
    "        training data array of targets\n",
    "    X_test: np.array\n",
    "        testing/validation data 2D array of features\n",
    "    y_test: np.array\n",
    "        testing/validation data array of targets \n",
    "    \"\"\"\n",
    "    \n",
    "    def overlap_windows(dataset, timesteps, steps_ahead):\n",
    "        \"\"\" Create overlaping window of time-series data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: pd.DataFrame\n",
    "            time-series pandas dataset\n",
    "        timesteps: int\n",
    "            number of time steps from the past for creating output arrays\n",
    "        steps_ahead: int\n",
    "            number of time steps into the future for making predictions\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X, y: np.array\n",
    "            input and output 3-d arrays of overlaping time windows\n",
    "        \"\"\"\n",
    "        X = []; y = []\n",
    "        \n",
    "        start = 0\n",
    "        for i in range(len(dataset)):\n",
    "            # Define the end of the input sequence\n",
    "            in_end = start + timesteps\n",
    "            out_end = in_end + steps_ahead\n",
    "            # Ensure that there is enough data\n",
    "            if out_end <= len(dataset):\n",
    "                X.append(dataset[start:in_end, :])\n",
    "                # First column holds load values\n",
    "                y.append(dataset[in_end:out_end, 0])\n",
    "            # Move along one time step\n",
    "            start += 1\n",
    "            \n",
    "        # Convert list to np.array\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "\n",
    "    data = dataframe.copy()\n",
    "    \n",
    "    if window_days*24 > data.values.shape[0]:\n",
    "        raise ValueError('Variable window_days has too large value: {}*24h = {} > {}, \\\n",
    "            which is more than there is data!'.format(window_days, window_days*24, \n",
    "                                                      data.values.shape[0]))\n",
    "    \n",
    "    # Training period\n",
    "    # ---------------\n",
    "    train_percent = train_percent/100.\n",
    "    st = pd.to_datetime(start_date)  # start date\n",
    "    et = st + dt.timedelta(days=int(train_percent*window_days))  # end date\n",
    "    train = data.loc[st:et].values\n",
    "    \n",
    "    # Standardize and transform training data set\n",
    "    mean_std_values = {}\n",
    "    for i, column in enumerate(columns):\n",
    "        # Calculate mean and standard deviation only\n",
    "        # from the training data set values\n",
    "        mu = train[:,i].mean()  # axis=0\n",
    "        sd = train[:,i].std()\n",
    "        mean_std_values[column] = (mu, sd)\n",
    "        # Standardize training data\n",
    "        train[:,i] = (train[:,i] - mu)/sd\n",
    "    \n",
    "    # Create overlapping windows with training data\n",
    "    X_train, y_train = overlap_windows(train, timesteps, steps_ahead)\n",
    "    \n",
    "    # Testing / Validation period\n",
    "    # ---------------------------\n",
    "    sv = et \n",
    "    ev = sv + dt.timedelta(days=int((1-train_percent)*window_days)+1)\n",
    "    test = data.loc[sv:ev].values\n",
    "    \n",
    "    # Transform testing/validation data set\n",
    "    for i, column in enumerate(columns):\n",
    "        # Use mean and standard deviation from the\n",
    "        # training data set\n",
    "        mu = mean_std_values[column][0]\n",
    "        sd = mean_std_values[column][1]\n",
    "        # Standardize test data\n",
    "        test[:,i] = (test[:,i] - mu)/sd\n",
    "    \n",
    "    # Create overlaping windows with test data\n",
    "    X_test, y_test = overlap_windows(test, timesteps, steps_ahead)\n",
    "    \n",
    "    return mean_std_values, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2p1LE6O4dsbd"
   },
   "outputs": [],
   "source": [
    "# Day-ahead (i.e. 24-hours ahead) short-term load prediction with time-series data\n",
    "START_DATE = '2014-01-09'  # starting date of the time-series data\n",
    "HISTORY_SIZE = 7*24        # window size in hours for the past history values (7*24=168)\n",
    "STEPS_AHEAD = 24           # hours-ahead for prediction\n",
    "WINDOW_SIZE_DAYS = 300     # window size in days for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoeVjagVdsbg"
   },
   "outputs": [],
   "source": [
    "# Training, Validation and Testing data sets\n",
    "mean_std_values, X_train, y_train, X_test, y_test = samples_timesteps_features(\n",
    "    dataframe, ['Load', 'Temperature'], \n",
    "    start_date=START_DATE, \n",
    "    timesteps=HISTORY_SIZE, \n",
    "    steps_ahead=STEPS_AHEAD,\n",
    "    window_days=WINDOW_SIZE_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unzqgB1pdsbi"
   },
   "outputs": [],
   "source": [
    "print('Training data shape:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Validation/Testing data shape:')\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "otXh-YcPdsbl"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256       # batch size\n",
    "EPOCHS = 20            # number of epochs for training\n",
    "STEPS_PER_EPOCH = 100  # steps per epoch for training\n",
    "VAL_STEPS = 50         # validation steps during training\n",
    "WAIT = 10              # patience for early stopping\n",
    "LR = 1e-3              # learning rate\n",
    "BUFFER_SIZE = 10000    # buffer size for shuffling batches using tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRVsmk8udsbp"
   },
   "outputs": [],
   "source": [
    "# Convert arrays using tf.data.Dataset\n",
    "# Training dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# Cache data, shuffle using a buffer, create batches while droping \n",
    "# a remainder of the data and finally repeat indefinitely as needed\n",
    "# by the steps_per_epoch parameter for the number of epochs specified.\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
    "# Validation dataset\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_data = test_data.batch(BATCH_SIZE, drop_remainder=True).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional (Conv1D) deep ANN using functional `tf.keras` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras functional API\n",
    "input_layer = keras.layers.Input(shape=(X_train.shape[1:]), batch_size=BATCH_SIZE)\n",
    "# set return_sequences=True if there is more than one LSTM layer\n",
    "x = keras.layers.Conv1D(filters=64, kernel_size=16, activation='relu')(input_layer)\n",
    "x = keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "x = keras.layers.Conv1D(filters=32, kernel_size=16, activation='relu')(x)\n",
    "x = keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(units=256, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "output_layer = keras.layers.Dense(STEPS_AHEAD)(x)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "adam = keras.optimizers.Adam(learning_rate=LR, decay=LR/EPOCHS)\n",
    "model.compile(loss='mae', optimizer=adam, metrics=['mape'])\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, \n",
    "                                           patience=WAIT, verbose=1,\n",
    "                                           restore_best_weights=True)\n",
    "history = model.fit(train_data, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_data=test_data, validation_steps=VAL_STEPS,\n",
    "                    callbacks=[early_stop], shuffle=False, verbose=1,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OvQBIPfdsbs"
   },
   "source": [
    "#### Convolutional (LSTM) deep ANN using functional `tf.keras` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ek-QOk9ydsbt"
   },
   "outputs": [],
   "source": [
    "# Convolutional two-layer deep ANN without regularization\n",
    "# tf.keras functional API\n",
    "input_layer = keras.layers.Input(shape=(X_train.shape[1:]), batch_size=BATCH_SIZE)\n",
    "# set return_sequences=True if there is more than one LSTM layer\n",
    "x = keras.layers.LSTM(units=16, return_sequences=True, activation='relu')(input_layer)\n",
    "x = keras.layers.LSTM(units=16, activation='relu')(x)\n",
    "output_layer = keras.layers.Dense(STEPS_AHEAD)(x)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    LR, decay_steps=EPOCHS, decay_rate=0.96, staircase=False)\n",
    "adam = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(loss='mae', optimizer=adam, metrics=['mape'])\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, \n",
    "                                           patience=WAIT, verbose=1,\n",
    "                                           restore_best_weights=True)\n",
    "history = model.fit(train_data, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_data=test_data, validation_steps=VAL_STEPS,\n",
    "                    callbacks=[early_stop], shuffle=False, verbose=1,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzyhg6hkdsbw"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "print('MAE val_loss at final epoch is {:.2f}, while min. val_loss is {:.2f}.'\n",
    "      .format(val_loss[-1], min(val_loss)))\n",
    "plt.plot(loss, label='training')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0u5jEA0dsby"
   },
   "outputs": [],
   "source": [
    "# Save a trained model (architecture and weights) on disk.\n",
    "# This model can be loaded and used without re-training:\n",
    "# model = keras.models.load_model('model-dense-dnn.tf')\n",
    "# for making predictions, or it can be restored and tra-\n",
    "# ining of the model can be continued from where it was.\n",
    "model.save('model-lstm-dnn.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRxIumx1dsb0"
   },
   "outputs": [],
   "source": [
    "for X, y in test_data.take(4):\n",
    "    # Predict using model\n",
    "    y_hat = model.predict(X)[0]\n",
    "\n",
    "    # Convert back to the original scale\n",
    "    y_predicted = mean_std_values['Load'][0] + y_hat*mean_std_values['Load'][1]\n",
    "    y_true = mean_std_values['Load'][0] + y[0]*mean_std_values['Load'][1]\n",
    "    # Calculate MAPE value\n",
    "    mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "    print('MAPE = {:.2f} (%)'.format(mape))\n",
    "    \n",
    "    # Plot predictions against true values\n",
    "    plt.plot(y_predicted, label='Prediction')\n",
    "    plt.plot(y_true, label='Actual')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWlCPCxidsb4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "data_log.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
